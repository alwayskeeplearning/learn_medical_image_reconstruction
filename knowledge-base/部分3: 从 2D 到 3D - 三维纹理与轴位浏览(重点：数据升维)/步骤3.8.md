# 步骤3.8: 实践 - 鼠标滚轮与拖拽联动，实现流畅滚动浏览

这是让我们的切片查看器“活”起来的关键一步，我们将两种核心的交互方式——鼠标滚轮和鼠标拖拽——赋予了应用，实现了流畅的切片浏览。

## 1. 鼠标滚轮滚动

### 理论讲解

我们通过监听用户的鼠标滚轮事件，并将这个滚动动作转化为切片索引的改变来实现交互。

过程分解如下：

1.  **事件监听**: 在渲染区域（Canvas 元素）上监听鼠标的 `wheel` 事件。
2.  **数据更新**: 从 `wheel` 事件中获取 `deltaY` 值，它表示滚轮滚动的方向和幅度。我们根据其正负来增加或减少当前显示的切片索引，并确保索引值不会超出有效范围。
3.  **Uniform 更新**: 将新的切片索引值更新到 `ShaderMaterial` 的 `uSlice` 这个 `uniform` 变量中。

一旦 `uSlice` 被更新，Three.js 的渲染循环在下一帧就会将新值传递给 GPU，Fragment Shader 会使用新的 `uSlice` 值去采样三维纹理中对应的切片，从而高效地在屏幕上显示出新的图像。

**数据流**:
`鼠标滚轮 -> WheelEvent -> deltaY -> 更新JS中的slice变量 -> 更新Three.js Uniform -> GPU -> Fragment Shader -> 渲染新切片`

### 核心 API

- **`element.addEventListener('wheel', (event) => { ... })`**: 用于在 Canvas 元素上监听鼠标滚轮事件。
  - `event.deltaY`: `WheelEvent` 对象的核心属性，正值表示向下滚动，负值表示向上滚动。
  - `event.preventDefault()`: 调用此方法可以阻止滚轮滚动的默认行为（如滚动整个网页）。
- **`material.uniforms.uSlice.value`**: 我们与着色器沟通的桥梁。通过直接修改 `.value` 属性，就可以将新的切片索引传递给 GPU。
- **`Math.max()` 和 `Math.min()`**: 用于将切片索引“钳制”在 `[0, totalSlices - 1]` 这个安全范围内，防止越界。

### 关键问题探讨：为何在 GLSL 中进行归一化？

在实践中，我们遇到了一个关键的设计抉择：应该在 JavaScript 中计算好归一化的切片坐标（`0.0` 到 `1.0`）再传入着色器，还是直接传入原始的切片索引（如 `90`），然后在着色器内部进行归一化？

我们选择了后者，原因如下：

| 对比项         | **方法一：传入索引，GLSL归一化 (我们的选择)** | **方法二：JS中归一化，GLSL直接使用**        |
| :------------- | :-------------------------------------------- | :------------------------------------------ |
| **JS 可读性**  | **高** (操作直观的整数索引 `90`)              | 低 (操作不直观的浮点数 `0.50279...`)        |
| **关注点分离** | **清晰** (JS管状态，GLSL管渲染)               | 职责混合                                    |
| **数据灵活性** | **高** (着色器同时拥有索引和总数)             | 低 (着色器只知道归一化坐标，丢失了原始信息) |
| **性能**       | 理论上略低 (GPU多一次除法)                    | 理论上略高                                  |
| **推荐度**     | **强烈推荐**                                  | 不推荐                                      |

**结论**: 为了代码的清晰性、可维护性和未来扩展性，牺牲几乎可以忽略不计的性能开销是完全值得的。这是一种更健壮、更专业的软件设计实践。

---

## 2. 鼠标拖拽滚动

在实现了滚轮交互后，我们增加了更为常用和直接的鼠标拖拽功能。

### 理论讲解

拖拽交互的核心在于**状态管理**，我们需要明确知道用户当前是否“正在拖拽”。

整个流程分解如下：

1.  **开始拖拽 (`mousedown`)**:

    - 当用户在图像上按下鼠标左键，我们将一个状态标志（例如 `isDragging`）设为 `true`，并记录下当前鼠标的起始垂直坐标 (`Y`)。

2.  **正在拖拽 (`mousemove`)**:

    - 此事件会持续触发，但我们只在 `isDragging` 为 `true` 时才执行逻辑。
    - 我们计算鼠标在垂直方向上移动的距离：`deltaY = 当前鼠标Y坐标 - 上一次记录的Y坐标`。
    - 根据 `deltaY` 来更新 `currentSliceIndex`。为了控制平滑度和灵敏度，我们引入了一个**速度因子**（`pixelsPerSlice`），表示“拖拽多少像素才切换一张切片”。
    - 更新完切片索引后，立即将当前鼠标的 Y 坐标存为“上一次记录的Y坐标”，为下一次计算做准备。

3.  **结束拖拽 (`mouseup` / `mouseleave`)**:
    - 当用户松开鼠标左键，或鼠标移出图像区域时，我们将 `isDragging` 标志设回 `false`，拖拽交互结束。

### 实现细节

- **`dragState` 对象**: 我们创建了一个全局对象来统一管理拖拽状态：
  - `isDragging: boolean`: 是否正在拖拽。
  - `previousMouseY: number`: 上一次鼠标的Y坐标。
  - `pixelsPerSlice: number`: 灵敏度因子，可直接修改此值来调整滚动速度。
  - `accumulatedDelta: number`: 像素移动累加器。我们将每次 `mousemove` 的 `deltaY` 累加起来，当累加值超过 `pixelsPerSlice` 阈值时才触发一次或多次切片更新。这种方式可以提供更平滑、精确的拖拽体验。
- **事件监听**: 我们为 `mousedown`, `mousemove`, `mouseup`, `mouseleave` 分别绑定了事件监听器来完整地实现上述逻辑。特别地，`mouseup` 事件被绑定在 `window` 对象上，以确保在画布外松开鼠标也能正确结束拖拽。

---

## 核心概念追问与深化：坐标系解惑

在实践过程中，我们对 Three.js 的 `OrthographicCamera` 和 WebGL 底层坐标系的关系产生了疑问，这部分内容对于理解渲染管线至关重要。

### 问题一：`OrthographicCamera` 的单位到底是什么？

**困惑点**：
为什么在初始化相机时使用 `new OrthographicCamera(-1, 1, 1, -1, ...)` 这样的归一化值，而在 `onWindowResize` 方法中又根据图像的实际像素尺寸（如 512x512）去设置 `camera.left = -256`, `camera.right = 256` 等值？这两种单位看起来相互矛盾。

**解答**：
`OrthographicCamera` 的 `left`, `right`, `top`, `bottom` 参数描述的**始终是世界空间单位 (World Space Units)**。

这两种看似矛盾的操作，实际上是两个不同阶段的行为：

1.  **阶段一：初始化 (Initialization)**

    - **代码**: `new OrthographicCamera(-1, 1, 1, -1, 0.1, 100);`
    - **目的**: 提供一个临时的、有效的、非零的初始视口。在构造相机对象时，我们还不知道要显示内容的确切尺寸，所以使用一个最常规的 `(-1, 1)` 范围作为**占位符**。这就像在拿到照片前，先准备一个默认比例的相框。

2.  **阶段二：动态调整 (Dynamic Adjustment)**
    - **代码**: 在 `onWindowResize()` 中根据内容（`Plane`）的真实尺寸和容器的宽高比，动态计算 `newCameraWidth` 和 `newCameraHeight`，并更新到相机的 `left/right/top/bottom`。
    - **目的**: 让相机视口完美匹配内容和窗口，防止图像变形。当我们的 `Plane` 以 512x512 的像素尺寸创建后，我们的世界单位就与像素形成了 1:1 的对应。`onWindowResize()` 的职责就是精确地调整相机这个“相框”，让它正好能容纳 512x512 的内容，并适应屏幕比例。

**总结**: 最终起作用的，**永远是 `onWindowResize()` 中动态计算出的、以世界空间单位（在我们的案例中就是像素）为基准的尺寸**。初始化的 `(-1, 1, 1, -1)` 在 `onWindowResize()` 第一次被调用后，就完成了它的历史使命。

### 问题二：相机定义的世界坐标系与 WebGL 的 `-1` 到 `1` 坐标系有什么关系？

**困惑点**：
既然相机使用的是我们自己定义的世界单位（如 `-256` 到 `256`），那么它和 WebGL 底层强制要求的 `-1` 到 `1` 的标准化设备坐标系（NDC）是如何关联起来的？

**解答**：
连接这两者的“魔法”，就是 **`projectionMatrix` (投影矩阵)**。

我们可以把一个顶点的“旅程”分为以下几站：

1.  **世界空间 (World Space)**: 这是我们自己定义的空间，单位是像素。例如，一个 512x512 平面右上角的顶点坐标是 `(256, 256, 0)`。
2.  **投影变换 (在顶点着色器中)**:
    - **核心代码**: `gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);`
    - **关键角色**: `projectionMatrix`。Three.js 根据你设置的 `camera.left/right/top/bottom` 参数，自动计算出这个矩阵。它的**唯一使命**就是：把所有在 `left/right/top/bottom` 范围内的世界坐标，**线性地映射**到 `-1` 到 `1` 的范围内。
    - **结果**: 世界坐标 `(256, 256, 0)` 经过变换后，变成了 `(1, 1)`。世界坐标 `(-256, -256, 0)` 变换后则成了 `(-1, -1)`。
3.  **标准化设备坐标系 (NDC)**: 这是 WebGL 的内部标准。所有 `gl_Position` 计算结果中，`x, y, z` 坐标都在 `[-1, 1]` 范围内的顶点才会被保留并进入下一步。
4.  **屏幕空间 (Screen Space)**: GPU 在幕后将 NDC 坐标再次线性映射成最终的屏幕像素坐标，例如 NDC 的 `(-1, -1)` 对应 Canvas 画布的左下角。

**投影仪类比**:

- **世界空间**: 桌子上的一张**幻灯片**（我们的 `Plane`）。
- **相机设置**: 调整投影仪的**镜头**，正好完整覆盖幻灯片（`camera.left/right` 等）。
- **投影矩阵**: 投影仪内部的**光学系统**，把捕捉到的内容转成标准光束。
- **NDC 空间**: 从投影仪射出的**标准形态的光束**（`-1` 到 `1` 的标准）。
- **屏幕空间**: 光束最终打到的**幕布**（我们的 `<canvas>`）。

**最终总结表**：

| 概念         | 作用                                           | 在我们代码中的体现                        |
| :----------- | :--------------------------------------------- | :---------------------------------------- |
| **世界空间** | 定义物体的大小和位置                           | `new PlaneGeometry(512, 512)`             |
| **相机视锥** | 决定要渲染世界的哪个部分                       | `camera.left = -256; camera.right = 256;` |
| **投影矩阵** | **“翻译官”**：将相机看到的内容翻译成 NDC       | `projectionMatrix` (由 Three.js 自动生成) |
| **NDC 空间** | WebGL 的**“标准语言”**，所有可见物体的最终归宿 | `-1` 到 `1` 的立方体                      |
| **屏幕空间** | 最终显示的物理像素                             | 你的 `<canvas>` 元素                      |

因此，`camera` 的单位是我们为方便思考而定义的“世界单位”，而 WebGL 的 `-1` 到 `1` 是它内部的“标准化单位”。`projectionMatrix` 就是连接我们和 WebGL 之间的、至关重要的桥梁。
