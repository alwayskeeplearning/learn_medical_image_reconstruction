# 步骤4.4 实践拓展：三视图MPR重构与对比学习

## 1. 重构动机：为十字线工具准备舞台

在正式进入十字线工具（`Crosshair`）的理论学习前，我们意识到，一个将所有视图渲染在同一个3D空间内的MPR实现，不利于后续的二维交互。一个真正的医学影像工作站，其核心是独立但联动的多视图（三视图）布局。

因此，我们决定先暂停原计划，动手将之前的统一3D空间MPR，重构为一个经典的三视图（Axial, Sagittal, Coronal）正交MPR布局，为后续所有交互功能打下坚实的基础。

## 2. 方案一：单 `Renderer` + 多 `Viewport` 实现

这是我们实现的第一版，也是性能更优、更工业化的版本。

### 2.1. 核心架构

- **HTML 结构**：使用CSS Grid或Flexbox创建三个独立的`<div>`容器，用于在页面上“占位”，规划出三个视图的布局区域。
- **渲染机制**：
  - 全局只创建一个`WebGLRenderer`实例和一个`<canvas>`元素。
  - 该`<canvas>`通过CSS绝对定位，覆盖在所有`<div>`的上层，像一张透明的画板。
  - **为每个视图创建独立的`Scene`和`OrthographicCamera`**。
  - 在渲染循环（`animate`函数）中，依次为每个视图执行以下操作：
    1.  通过`div.getBoundingClientRect()`获取其在页面上的位置和大小。
    2.  调用`renderer.setViewport()`和`renderer.setScissor()`，将渲染区域限定在这个`div`对应的矩形内。
    3.  使用该视图对应的`camera`渲染对应的`scene`。

### 2.2. 关键API深度解析：`setViewport` vs `setScissor`

这是理解该方案的钥匙。

- **`renderer.setViewport(x, y, width, height)`**:

  - **作用**: 定义“照片”的位置和大小。它告诉渲染器，把相机拍摄到的内容，最终**贴在**画布的哪个矩形区域里。
  - **类比**: 像在PPT里调整一张图片的位置和大小。
  - **效果**: 它负责将三维场景**映射**到二维画布的指定区域。

- **`renderer.setScissor(x, y, width, height)`**:
  - **作用**: 定义“裁剪”的区域。它告诉渲染器，**只允许**在这个矩形区域内进行绘制，超出部分将被丢弃。
  - **类比**: 像盖在画纸上的镂空模板，只允许在镂空处作画。
  - **关键配合**: `setScissor`最重要的作用是**保护其他区域不被`renderer.clear()`意外擦除**。当开启`scissor test`后，`clear`操作只会清空裁剪框内的区域，从而保证了多个视图在同一画布上渲染时互不干扰。

**总结表格**

| 函数                     | 作用                                     | 类比                        | 关键点                                  |
| :----------------------- | :--------------------------------------- | :-------------------------- | :-------------------------------------- |
| `renderer.setViewport()` | **映射**：把场景输出“贴”到画布的指定位置 | 在PPT里设置图片的位置和大小 | 决定了最终图像的**显示区域**            |
| `renderer.setScissor()`  | **裁剪**：限制绘制操作的“作画区域”       | 盖在画纸上的镂空模板        | 保护其他区域不被 `clear` 等操作意外擦除 |

### 2.3. `Resize` (窗口缩放) 逻辑

多视图下的`resize`比单视图复杂，核心是保证医学影像**不因窗口或`div`的宽高比变化而变形**。

1.  **`handleResize()` (宏观)**: 监听窗口变化，只做两件事：更新全局`renderer`的尺寸，然后通知每个视图去独立更新自己。
2.  **`updateView()` (微观)**: 每个视图独立执行：
    a. 获取自身`div`的宽高。
    b. 获取自身渲染的医学切片的物理宽高。
    c. **比较`div`和切片的宽高比**，以决定是“以宽为准”还是“以高为准”来计算相机的视锥体，从而确保图像始终保持正确的比例，多余的部分留出黑边。
    d. 更新正交相机的`left`, `right`, `top`, `bottom`属性。

## 3. 方案二：多 `Renderer` + 共享 `Texture` (对比学习)

为了深化理解，我们实现了第二套方案作为对比。

### 3.1. 核心架构

- **HTML 结构**：每个视图`<div>`内部都包含一个自己的`<canvas>`元素。
- **渲染机制**：
  - **为每个视图创建独立的`WebGLRenderer`**，并绑定到其专属的`<canvas>`上。
  - `Scene`和`Camera`依然是每个视图一套。
  - **`Data3DTexture`只创建一次**，然后将其引用共享给三个视图的`ShaderMaterial`。WebGL允许在同一页面进程的不同渲染上下文之间共享GPU资源。
  - `animate`函数变得极其简单，只需遍历所有视图，调用各自的`renderer.render()`即可，不再需要`viewport`和`scissor`。
  - `resize`逻辑也相应地分散到每个视图中，各自管理自己的`renderer`尺寸和`camera`视锥体。

### 3.2. 两种方案对比总结

| 特性           | 单 Renderer 方案 (方案一)                                    | 多 Renderer 方案 (方案二)                                 |
| :------------- | :----------------------------------------------------------- | :-------------------------------------------------------- |
| **代码复杂度** | `animate` 和 `resize` 逻辑较复杂 (需要 `viewport`/`scissor`) | `animate` 和 `resize` 逻辑更分散、更直观                  |
| **初始化开销** | **低**。只创建一个 WebGL 上下文。                            | **高**。创建三个独立的 WebGL 上下文，每个都有相当的开销。 |
| **运行时性能** | **高**。GPU 无需切换上下文，效率更高。                       | **较低**。频繁切换 WebGL 上下文可能会带来性能损耗。       |
| **资源管理**   | 简单，所有资源都在一个上下文中。                             | 需要注意跨上下文的资源共享。                              |
| **适用场景**   | 高性能、复杂布局的专业应用。                                 | 简单应用、快速原型或页面中不同组件需要隔离渲染的场景。    |

## 4. 关键修正：`Slice Thickness` vs `Spacing Between Slices`

在实践中，我们发现冠状位和矢状位视图有被“压扁”的现象。这暴露出一个DICOM数据处理中的常见陷阱。

- **问题根源**: 代码最初使用`Slice Thickness` (层厚) 乘以切片数来计算体数据的总高度。但这忽略了**层间距**可能与层厚不同。
- **三种扫描方式**:
  1.  **有间隙扫描 (Gapped)**: `层厚 < 层间距` (我们遇到的情况)
  2.  **连续扫描 (Contiguous)**: `层厚 = 层间距`
  3.  **重叠扫描 (Overlapping)**: `层厚 > 层间距` (用于高质量三维重建)
- **最终的稳健方案**: 放弃依赖不一定可靠的`Slice Thickness`标签，改为通过已排序切片的`ImagePositionPatient`标签，**直接计算相邻两张切片中心点的三维空间距离**。
  ```typescript
  const pos0 = new THREE.Vector3().fromArray(sortedSlices[0].imagePosition);
  const pos1 = new THREE.Vector3().fromArray(sortedSlices[1].imagePosition);
  zSpacing = pos0.distanceTo(pos1);
  ```
  这个`zSpacing`是物理世界中真实的层间距，不受扫描方式影响。使用它来计算体数据的总物理尺寸和变换矩阵，保证了所有视图的渲染比例都是精确无误的。这一修正极大地提升了代码的健壮性和准确性。

<!-- 层厚	层间距	10层图像	Z轴总长度计算
情况1	1mm	1mm	10层	(10 - 1) * 1mm + 1mm = 10mm
情况2	1mm	2mm	10层	(10 - 1) * 2mm + 1mm = 19mm
情况3	1mm	0.5mm	10层	(10 - 1) * 0.5mm + 1mm = 5.5mm -->
<!--
1. 关于是否有必要修正代码？
我的建议是：没有必要修正，当前的代码实现是更符合三维可视化实践的。
这似乎有点反直觉，我来解释一下为什么“近似”反而是“更好”的：
我们所有三维渲染的基础，是那个 Data3DTexture。这个纹理在概念上是一个 完全规整、均匀的立体网格，就像一个魔方。在这个模型里，每一个小方块（体素）在Z轴方向的尺寸都必须是 相同 的。
这个“Z轴尺寸”，就是我们用 distanceTo 计算出来的 层间距 (accurateZSpacing)。
我们所做的，就是告诉GPU：“嘿，我有一个三维体数据，它每个体素的物理尺寸是 pixelSpacing[0] x pixelSpacing[1] x accurateZSpacing 毫米。”
voxelToPatientMatrix 这个矩阵的核心作用，就是把这个 (i, j, k) 的体素索引，根据上述的物理尺寸，正确地映射到真实世界（病人坐标系）中去。
如果我们强行使用 (n-1) * spacing + thickness 这个公式，会发生什么？
这个公式计算的是整个数据体从第一片图像的“顶/头端”到最后一片图像的“底/脚端”的总长度。它承认了“间隙”和“厚度”是两个概念。但这恰恰破坏了我们“均匀网格”这个根本模型。我们的三维纹理采样、我们的矩阵变换，都是基于“网格是均匀的”这个大前提。
所以，结论是：
为了进行三维渲染和MPR，我们必须把真实世界中可能不均匀的扫描（比如层间距和层厚不同）抽象成一个均匀的体素网格。在这个抽象过程中，使用“层间距”作为这个网格在Z轴的统一间距，是最合理、最能还原空间位置关系的做法。
因此，physicalSize.z = zSpacing * dimensions.depth 这个计算，虽然在“总长度”上是近似，但在定义“渲染模型”上是正确的。 -->

<!-- 这段代码是干什么呢？总体思路我懂 就是8个点到切面的点积投影 但是uv为啥是这么算的？不应该直接用对应的x轴和y轴嘛？这是自己算了一遍？
目标：为任意3D平面建立一个2D坐标系
想象一下，你在太空中悬浮着一张纸（我们的MPR平面）。这张纸可以朝向任何方向。它的朝向由它的法向量 viewNormal 决定。
现在，你想在这张纸上画一个2D的网格，你需要确定这张纸上的 X 轴和 Y 轴在哪里。我们把这两个轴称为 u 轴（平面的本地X轴）和 v 轴（平面的本地Y轴）。
这三个向量 viewNormal、u 和 v 必须满足以下条件，才能构成一个标准的坐标系：
它们必须相互垂直（正交）。
它们的长度都必须为1（单位向量）。
为什么不能直接用世界的X和Y轴？
你的想法“不应该直接用对应的x轴和y轴嘛？”是非常自然的。
对于轴位（Axial）平面，它的法向量是 (0, 0, 1)（世界Z轴）。此时，它的本地 u 轴和 v 轴确实可以就是世界X轴 (1, 0, 0) 和世界Y轴 (0, 1, 0)。
但是，对于冠状位（Coronal）平面，它的法向量是 (0, 1, 0)（世界Y轴）。它的本地 u 轴和 v 轴就变成了世界X轴 (1, 0, 0) 和世界 -Z 轴 (0, 0, -1)。
对于矢状位（Sagittal）平面，它的法向量是 (1, 0, 0)（世界X轴），情况就又不一样了。
如果我们为每种视图都写死一套 u 和 v 轴，代码会变得复杂且缺乏通用性。将来我们要做任意斜切平面时，就完全没法用了。
因此，我们需要一个通用的算法：给我任意一个 viewNormal，我都能立刻计算出与它垂直的 u 和 v。
这段代码是如何做到“通用计算”的？
这就是这段代码的精髓所在，它利用了向量数学的基本原理：
找到第一个垂直向量 u
核心原理：如果两个向量的点积为0，那么它们就相互垂直。
我们有一个向量 viewNormal = (nx, ny, nz)。我们需要找一个向量 u = (ux, uy, uz)，使得 nx*ux + ny*uy + nz*uz = 0。
这个方程有无数个解。代码中采用了一种非常聪明的“交换和变号”技巧来快速找到一个解：
u.set(-nz, 0, nx)：我们来验证一下它的点积：nx*(-nz) + ny*0 + nz*nx = -nx*nz + 0 + nx*nz = 0。完美！它确实是垂直的。
u.set(0, nz, -ny)：同样验证点积：nx*0 + ny*nz + nz*(-ny) = 0 + ny*nz - ny*nz = 0。也完美！
if/else 的作用：这是一个健壮性处理。如果 viewNormal 恰好是 (0, 1, 0)，那么第一种方法 (-nz, 0, nx) 会得到 (0, 0, 0)，一个零向量是无法作为坐标轴的。if/else 语句就是为了避免选到这种会产生零向量的公式，确保我们总能得到一个有效的 u 向量。
.normalize()：最后，我们把 u 变成单位长度，满足我们前面说的条件2。
找到第二个垂直向量 v
核心原理：两个向量的叉积（Cross Product）结果，会同时垂直于这两个原始向量。
现在我们已经有了两个相互垂直的单位向量：viewNormal 和 u。
我们只需要执行 v.crossVectors(viewNormal, u)，就能得到第三个向量 v。这个 v 天然就同时垂直于 viewNormal 和 u，并且由于前两者是单位向量，v 的长度也自动为1。
总结
所以，你选中的这段代码，是一个非常标准且高效的数学过程，可以被称为 “从一个向量构建一个正交基”。
它并没有“重复计算”，而是从无到有地、为任意一个3D平面，构建出了一套属于它自己的、可靠的2D坐标系（u 和 v）。有了这个坐标系，我们才能通过点积，将三维世界中的顶点，正确地“拍平”到这个平面上，计算出它们在这个平面上的2D坐标，进而算出最终的宽度和高度。 -->

<!-- 旋转公式 正是利用三角函数实现的二维平面中一个点 (P) 绕另一个中心点 (C) 旋转的经典数学公式。
直接看这个公式 npx = cx + (px - cx) * cos - (py - cy) * sin; 可能会觉得很困惑，但如果我们把它分解成三个步骤，就会非常清晰。
核心思想：先平移，再旋转，最后再平移回去
想象一下，你有一张纸，上面有一个点 P 和一个中心点 C。你想让 P 绕着 C 旋转30度。直接在纸上画这个弧线可能有点难。一个更简单的方法是：
第一步：把整张纸移动，让中心点 C 对准坐标系的原点 (0, 0)。
第二步：现在 P 在一个新的位置，我们绕着原点 (0, 0) 旋转 P。（绕原点旋转的公式非常简单）
第三步：旋转完成后，再把整张纸移回原来的位置。
rotatePoint 函数中的那两行代码，就是把这三个步骤合并在了一起。
公式分解
让我们把代码和上面三个步骤对应起来看：
px, py: 这是你要旋转的点 P 的坐标。
cx, cy: 这是旋转的中心点 C 的坐标。
angle: 旋转的角度（弧度制）。
cos, sin: 预先计算好角度的余弦和正弦值。
步骤 1: 将坐标系平移到以 C 为原点
在数学上，“把纸移动，让 C 成为原点”，就等于让 P 的坐标减去 C 的坐标。
px - cx：这是点 P 相对于中心 C 的 x 距离。
py - cy：这是点 P 相对于中心 C 的 y 距离。
我们暂时把这两个新坐标称为 dx 和 dy。
const dx = px - cx;
const dy = py - cy;
步骤 2: 绕新原点 (0,0) 旋转
现在我们可以使用最简单的二维旋转公式了。一个点 (x, y) 绕原点 (0,0) 旋转 θ 角度后，新坐标 (x', y') 的计算公式是：
x' = x * cos(θ) - y * sin(θ)
y' = x * sin(θ) + y * cos(θ)
把我们的 dx 和 dy 套进这个公式：
rotated_dx = dx * cos - dy * sin
rotated_dy = dx * sin + dy * cos
步骤 3: 将坐标系平移回去
旋转完成后，我们需要把坐标系移回原来的位置。在数学上，就是把刚刚减去的中心点坐标 (cx, cy) 再加回来。
npx = cx + rotated_dx
npy = cy + rotated_dy
合并公式
现在，我们把 rotated_dx 和 rotated_dy 替换掉：
npx = cx + (dx * cos - dy * sin)
npy = cy + (dx * sin + dy * cos)
再把 dx 和 dy 替换掉：
npx = cx + ((px - cx) * cos - (py - cy) * sin)
npy = cy + ((px - cx) * sin + (py - cy) * cos) -->
