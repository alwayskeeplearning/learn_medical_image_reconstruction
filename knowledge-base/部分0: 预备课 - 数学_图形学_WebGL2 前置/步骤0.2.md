# 步骤0.2: 理论 - GPU 渲染管线、纹理与颜色空间

## **1. GPU 渲染管线 (Simplified)**

你可以把渲染管线想象成一条高度专业化的工厂流水线。我们把 3D 模型的顶点数据（一堆坐标）作为原材料放进流水线的一端，最终在另一端得到成品的像素图像。这个流水线主要分为几个关键阶段，其中有两个阶段是我们可以通过编写小程序（称为**着色器 Shader**）来编程控制的：

1.  **顶点处理 (Vertex Processing)**:

    - **输入**: 模型的原始顶点坐标（通常在世界坐标系）。
    - **核心工作**: 对每个顶点执行一次**顶点着色器 (Vertex Shader)**。
    - **你的任务 (in Vertex Shader)**:
      - **坐标变换**: 将顶点从世界坐标系，通过相机变换、投影变换，最终转换到**归一化设备坐标系 (NDC)**。这是最核心的任务。
      - **传递数据**: 可以将一些顶点级别的数据（如颜色、法向量、纹理坐标）传递给下一个阶段。

2.  **光栅化 (Rasterization)**:

    - **输入**: 经过变换后的顶点（NDC 坐标）。
    - **核心工作**: GPU 在这个阶段会计算出由顶点组成的图元（通常是三角形）覆盖了屏幕上的哪些像素。它会为这些像素生成“**片元 (Fragment)**”。片元可以理解为一个“候选像素”，它包含了位置、颜色、纹理坐标等信息。
    - **这一阶段是硬件固定的，我们无法编程控制。**

3.  **片元处理 (Fragment Processing)**:

    - **输入**: 光栅化阶段生成的片元。
    - **核心工作**: 对每个片元执行一次**片元着色器 (Fragment Shader)**。
    - **你的任务 (in Fragment Shader)**:
      - **计算最终颜色**: 这是片元着色器的**唯一目标**。你可以根据纹理、光照、材质等信息，通过计算最终决定这个像素应该显示什么颜色。
      - **可以丢弃片元**: 你也可以决定某些片元不应该被绘制（例如，实现透明效果）。

4.  **输出合并 (Output Merging)**:
    - **输入**: 片元着色器输出的颜色。
    - **核心工作**: 将最终的颜色写入到屏幕的**帧缓冲区 (Framebuffer)** 中，同时进行深度测试（判断哪个物体在前面）、混合（处理半透明）等操作。最终我们就能在屏幕上看到图像了。

## **2. 纹理坐标 (Texture Coordinates)**

- **核心概念**: 纹理 (Texture) 就是一张贴图。为了告诉 GPU 如何将这张贴图“贴”到 3D 模型上，我们需要为模型的每个顶点指定一个**纹理坐标**。
- **坐标系统**: 纹理坐标通常用 `(u, v)` 表示，`u` 是横向，`v` 是纵向。它们的范围通常是 `[0, 1]`。`(0, 0)` 代表纹理的左下角，`(1, 1)` 代表纹理的右上角。
- **工作原理**: 在光栅化阶段，GPU 会根据顶点的纹理坐标自动为每个片元插值计算出它对应的纹理坐标。然后在片元着色器中，我们就可以用这个坐标去纹理上**采样 (Sample)**，取出对应位置的颜色值。

## **3. 颜色空间 (Color Space)**

- **核心概念**: 这是一个经常被忽略但非常重要的概念。计算机存储和计算颜色时的方式，与人眼感知颜色的方式、显示器显示颜色的方式并不完全相同。
- **线性空间 (Linear Space)**:
  - 这是进行**物理上正确的光照计算**所必需的颜色空间。在这个空间里，颜色值的大小与光的物理强度成**正比**。例如，`0.5` 的灰度就代表物理光强的 50%。我们所有的光照、混合计算都应该在线性空间中进行。
- **sRGB 空间 (Gamma Corrected Space)**:
  - 人眼对暗部的亮度变化比对亮部的更敏感。显示器为了优化显示效果，使用了伽马校正 (Gamma Correction)，使得颜色输出是非线性的。我们日常看到的大多数图片（`.jpg`, `.png`）和颜色值（如 CSS 里的 `#FF0000`）都是在 sRGB 空间下的。
- **我们的任务**:
  - **输入**: 当我们加载一张 sRGB 纹理时，需要告诉 GPU：“这张图是 sRGB 的，请在采样时自动将它转换到**线性空间**”。
  - **计算**: 在着色器中，我们所有的计算都在**线性空间**中完成。
  - **输出**: 在片元着色器输出最终颜色时，需要告诉 GPU：“这个计算结果是线性的，请在写入屏幕前将它转换回 **sRGB 空间**”，以确保在显示器上看起来是正确的。
  - 幸运的是，Three.js 在很大程度上为我们自动处理了这些转换，我们只需要正确设置 `Texture.encoding` 和 `WebGLRenderer.outputEncoding` 即可。

---

## **问答归档**

**问1: 展开讲讲什么是相机变换以及投影变换？为什么需要经过这些变换？变换完事儿就可以归一化[-1, 1]了？**

**答:**
您的这个问题直指 3D 渲染中的核心和易混淆之处。

### **什么是相机变换 (Camera/View Transform)？**

想象一下你在一个大房间（世界坐标系）里，房间里有很多家具（物体）。你想用手里的相机拍一张照片。你有两种方式来构图：

1.  **现实世界的方式**: 你自己移动，走到房间的某个角落，调整相机对准某个家具。
2.  **计算机图形学的方式**: 你（相机）永远站在原地不动（待在 `(0,0,0)`），而是**把整个房间和所有家具移动和旋转**，直到你想拍的那个家具正好出现在你的相机镜头前。

**相机变换**做的就是第二件事。它是一个纯粹为了计算方便而引入的技巧。它将场景中的所有物体从**世界坐标系**变换到一个新的坐标系——**相机/观察坐标系 (Camera/View Space)**。在这个新的坐标系里，相机永远位于原点 `(0,0,0)`，并且朝向 Z 轴的负方向。

### **什么是投影变换 (Projection Transform)？**

经过相机变换后，所有物体都已经是相对于相机的位置了，但它们仍然存在于一个三维空间中。可我们的屏幕是二维的。**投影变换**的任务就是模拟真实世界中光线通过相机镜头成像的过程，把这个三维的相机空间“压扁”成二维图像。

这个过程会定义一个**可视区域**，我们称之为**视锥 (View Frustum)**。它就像一个被切掉了顶部的金字塔，相机的镜头在这个金字塔的尖端。只有位于这个视锥内部的物体才最终是可见的。

投影变换的核心工作就是把这个**形状不规则的视锥**，精准地映射（挤压变形）成一个**形状规则的、边长为 2 的标准立方体**。这个立方体就是我们之前提到的**归一化设备坐标系 (NDC)**。

### **为什么需要这些变换？变换完就归一化了吗？**

- **为什么需要？**

  - **相机变换**：为了**确定观察点**。没有它，我们只能永远从世界的中心原点 `(0,0,0)` 看东西。有了它，我们才能在场景中自由地漫游。
  - **投影变换**：为了**实现 3D 到 2D 的转换**和**模拟透视**（近大远小的效果）。同时，它还为 GPU 提供了一个标准化的裁剪空间。

- **变换完就归一化了吗？**
  - **是的**。可以这么理解：`顶点着色器` 中最核心的公式就是 `gl_Position = ProjectionMatrix * ViewMatrix * ModelMatrix * vertexPosition;`。
  - 这个公式将一个模型的本地坐标，经过模型变换（放到世界里）、相机变换（放到相机前）、投影变换（压成标准立方体），最终得到的 `gl_Position` 就是在 NDC 空间（或其前身裁剪空间）中的坐标。这个坐标的 `x`, `y`, `z` 分量都在 `[-1, 1]` 的范围内（如果它可见的话）。超出这个范围的顶点和图元就会被 GPU 硬件自动**裁剪 (Clipping)** 掉。

---

**问2: 纹理坐标 (u,v) 的原点为什么在左下角，而不是像 Canvas2D 一样在左上角？**

**答:**
您的观察非常敏锐！Canvas2D、CSS、Photoshop 等等，它们的原点都在左上角。而 WebGL/Three.js 的纹理坐标原点却在左下角。

这是由两个不同的历史传统导致的：

1.  **图像/屏幕传统 (左上角为原点)**: 计算机显示器是从上到下、从左到右扫描像素来显示图像的。因此，几乎所有的 2D 图像格式（PNG, JPG）和 2D 图形 API（如 Canvas2D）都遵循这个约定，将图像数据的第一个像素放在内存的开始位置，对应图像的左上角。

2.  **数学/3D 图形学传统 (左下角为原点)**: 在数学的笛卡尔坐标系中，原点 `(0,0)` 位于左下角，Y 轴向上为正方向。早期的 3D 图形学 API（比如 OpenGL，WebGL 的前身）为了在数学上保持一致性，就采用了这个约定。从模型坐标、世界坐标到 NDC 坐标，它们的 Y 轴都统一指向上方。让纹理坐标系也遵循这个约定，可以避免在着色器中进行额外的坐标翻转计算，使得整个系统在数学上更加和谐统一。

### **有什么好处吗？**

遵循数学传统的主要好处是**一致性**。在 3D 场景中，所有坐标系的 Y 轴都指向上，这让开发者的心智模型更简单，不容易出错。

### **那如何解决这个矛盾呢？**

当我们将一张“左上角为原点”的图片加载到 WebGL 中，要用作“左下角为原点”的纹理时，该怎么办？

幸运的是，像 Three.js 这样的库已经为我们完美地处理了这个问题。当 Three.js 的 `TextureLoader` 加载一张图片时，它会默认设置一个属性：`texture.flipY = true`。

这个设置会告诉 Three.js 在将图片数据上传到 GPU 时，自动在垂直方向上进行一次翻转。这样一来，虽然原始图片文件是 Y 轴朝下的，但在 GPU 内部和我们的着色器代码中，我们看到的纹理数据已经是被正确翻转过的、Y 轴朝上的了，可以直接使用，我们作为开发者几乎感觉不到这个差异的存在。

---

**问3: 为什么顶点着色器中的公式 `gl_Position = P * V * M * v` 计算顺序是从右到左？以及这些参数的含义？**

**答:**
这个公式是顶点着色器的核心。

### **1. 各个参数的含义**

- `vertexPosition`: 这是一个**向量**（通常是 4x1 的齐次坐标列向量 `[x, y, z, 1]`），代表模型**最原始**的顶点坐标。这个坐标是相对于模型自己的中心点来定义的，我们称之为**模型/本地坐标 (Model/Local Space)**。

- `ModelMatrix`: 这是一个**变换矩阵**。它的作用是将 `vertexPosition` 从**模型坐标**变换到**世界坐标 (World Space)**。这个矩阵包含了模型在整个大场景中的**位置（平移）、旋转和缩放**信息。

- `ViewMatrix`: 也是一个**变换矩阵**。它的作用是将坐标从**世界坐标**变换到**相机/观察坐标 (View/Camera Space)**。它相当于把整个世界移动和旋转，使得相机刚好位于原点 `(0,0,0)` 并看向特定方向。

- `ProjectionMatrix`: 还是一个**变换矩阵**。它的作用是将坐标从**相机坐标**变换到**裁剪空间 (Clip Space)**。它定义了视锥，并为透视效果做准备。(裁剪空间经过 GPU 硬件的透视除法后，就变成了我们说的 NDC 空间)。

### **2. 计算顺序为什么是从右到左？**

**是的，计算顺序严格从右到左，这是由线性代数中矩阵乘法的运算法则决定的，它本质上是一种函数嵌套。**

在数学上，当你看到 `A * B * v` (其中 A, B 是矩阵, v 是向量) 时，正确的运算顺序是先计算 `B * v` 得到一个新的向量 `v'`，然后再计算 `A * v'`。

我们可以把每次矩阵乘以向量的运算看作是一次函数调用：

- `transform_M(v)` = `ModelMatrix * v`
- `transform_V(v)` = `ViewMatrix * v`
- `transform_P(v)` = `ProjectionMatrix * v`

那么我们的公式 `P * V * M * v`，就等价于函数嵌套调用的 `transform_P( transform_V( transform_M(v) ) )`。

这样写就非常清晰了：

1.  首先，对原始顶点 `v` 应用**模型变换**。
2.  然后，对上一步的结果应用**相机变换**。
3.  最后，对上一步的结果应用**投影变换**。

这个顺序与我们渲染管线的逻辑流程是完全一致的。所以，**“从右向左”的计算顺序，正是为了实现“从模型->世界->相机->裁剪空间”这一系列逻辑上正确的坐标变换流程**。

---

**问4: 为什么三维坐标要用一个四分量的齐次坐标 `(x, y, z, 1)` 来表示？**

**答:**
这正是计算机图形学为了工程上的优雅和效率而做出的最巧妙的设计之一。

### **什么是齐次坐标？**

齐次坐标是一种坐标表示法，它使用 N+1 个数字来表示一个 N 维空间中的点。对于我们关心的三维空间，我们就用 4 个数字 `(x, y, z, w)` 来表示。为了将一个普通的三维坐标 `(X, Y, Z)` 转换为齐次坐标，我们只需要设置 `w=1`，即 `(X, Y, Z, 1)`。

### **为什么要有它？解决了什么问题？**

**核心原因：它用一种统一的数学形式（矩阵乘法）解决了两种完全不同的几何变换（线性变换 和 平移变换）。**

1.  **线性变换 (旋转/缩放)**: 可以用一个 3x3 的矩阵与一个 3x1 的向量**相乘**来实现。

```
    [x'] [r1 r2 r3] [x]
    [y'] = [r4 r5 r6] [y]
    [z'] [r7 r8 r9] [z]
```

2.  **平移变换**: 是一个向量**加法**，无法用 3x3 矩阵的乘法来表示。

```
    [x'] [x] [tx]
    [y'] = [y] + [ty]
    [z'] [z] [tz]
```

    这个矛盾使得我们无法将所有变换（旋转、缩放、平移）预先合并成一个单独的矩阵，来高效地处理成千上万的顶点。

### **齐次坐标的“魔法”**

通过增加一个维度 `w`（并将其设为 1），我们就可以把**加法**也“伪装”成**乘法**。我们把矩阵扩展到 4x4，将平移量 `(tx, ty, tz)` 放在矩阵的最后一列。当这个 4x4 的平移矩阵去乘以我们的齐次坐标 `[x, y, z, 1]` 时：

```
[x']   [1, 0, 0, tx] [x]   [1*x + 0*y + 0*z + tx*1]   [x + tx]
[y'] = [0, 1, 0, ty] [y] = [0*x + 1*y + 0*z + ty*1] = [y + ty]
[z']   [0, 0, 1, tz] [z] = [0*x + 0*y + 1*z + tz*1] = [z + tz]
[w']   [0, 0, 0, 1 ] [1]   [0*x + 0*y + 0*z + 1*1 ]   [  1   ]
```

我们成功地将**平移这个加法操作，统一到了矩阵乘法的框架下**！

### **齐次坐标带来的额外好处**

1.  **统一所有变换**: 现在，旋转、缩放、平移都可以用一个 4x4 矩阵来表示了。这意味着我们可以把 `ProjectionMatrix`, `ViewMatrix`, `ModelMatrix` 预先在 CPU 端相乘，得到一个最终的 `MVP` 矩阵，极大提升了效率。
2.  **区分“点”和“方向向量”**:
    - 一个**点 (Position)**，我们用 `w=1` 表示：`(x, y, z, 1)`。它会受到平移影响。
    - 一个**方向向量 (Direction Vector)**，我们用 `w=0` 表示：`(vx, vy, vz, 0)`。它只有方向，不应该被平移。当你用平移矩阵去乘它时，平移部分乘以 `w=0`，所以平移无效，这完全符合物理直觉。
